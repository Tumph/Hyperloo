from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import json
import re
import time

<<<<<<< HEAD
def handle_login(driver):
    print("\nüîê Manual Login Required")
    print("1. Browser window opening...")
    print("2. Complete institutional login")
    print("3. Wait for redirect to Outline after login")
    driver.get('https://outline.uwaterloo.ca/browse/')
    
=======


# Set up the Selenium WebDriver (ensure the path to chromedriver is correct)
service = Service('Hyperloo/scrapers/scrapesyllabus/chromedriver.exe')  # Update this path
options = webdriver.ChromeOptions()
# options.add_argument('--headless')  # Run in headless mode if needed

def courses(url):
    driver = webdriver.Chrome(service=service, options=options)
    temp = []
>>>>>>> 02da7fde48ea5bbb8216f94a41c160796c560a5f
    try:
        #Waits until Course search element is open to start the script
        WebDriverWait(driver, 120).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "input[placeholder*='Course Search']"))
        )
        print("\n‚úÖ Login validated!")
        time.sleep(2)
    except Exception as e:
        print(f"\n‚ùå Login failed: {str(e)}")
        driver.quit()
        exit()


<<<<<<< HEAD
def extract_syllabus_data(soup):
    """Enhanced extraction with multiple fallback strategies"""
    print("üîç Beginning syllabus extraction...")
    topics = []
    times = []
    
    # Strategy 1: Find schedule section using flexible header detection
    schedule_header = soup.find(['h2', 'h3', 'h4'], 
        string=re.compile(r'schedule|week|content week|tentative', re.IGNORECASE))
    
    if schedule_header:
        print(f"üìë Found schedule header: {schedule_header.get_text(strip=True)}")
        table = schedule_header.find_next('table')
        
        if table:
            print("üìä Processing schedule table")
            time_col, topic_col = get_column_indices(table)
            print(f"üîß Detected columns - Time: {time_col+1}, Topics: {topic_col+1}")
            
            rows = table.find_all('tr')[1:]  # Skip header row
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) > max(time_col, topic_col):
                    time_text = cells[time_col].get_text(strip=True)
                    topic_text = cells[topic_col].get_text(strip=True)
                    
                    if time_text and topic_text:
                        times.append(time_text)
                        topics.append(topic_text)
            print(f"‚úÖ Table extraction: {len(topics)} items found")
    
    # Strategy 2: Fallback to list items if table parse fails
    if not topics:
        print("‚ö†Ô∏è Table parse failed, trying list items...")
        list_items = soup.select('ul li, ol li')
        for item in list_items:
            if re.match(r'(week|session)\s+\d+', item.text, re.IGNORECASE):
                parts = re.split(r':\s*', item.text, 1)
                if len(parts) == 2:
                    times.append(parts[0])
                    topics.append(parts[1])
        print(f"üìù List extraction: {len(topics)} items found")
    
    # Remove duplicates while preserving order
    seen = set()
    topics = [x for x in topics if not (x in seen or seen.add(x))]
    seen = set()
    times = [x for x in times if not (x in seen or seen.add(x))]
    
    return topics, times

def scrape_syllabi():
    print("üöÄ Starting syllabus scraping process")
    with open('../coursescraper/courses.json', 'r') as f:
        courses = json.load(f)
    
    driver = webdriver.Chrome()
    #Wait for login to execute scraping
    handle_login(driver)
    syllabus_data = []

    for idx, course in enumerate(courses):
        print(f"\nüìö Processing {idx+1}/{len(courses)}: {course['course_code']}")
        formatted_code = re.sub(r'(\D)(\d)', r'\1 \2', course['course_code'])
        
        try:
            # Navigate and search
            print("üåê Navigating to search page...")
            driver.get('https://outline.uwaterloo.ca/browse/')
            
            print(f"üîé Searching for {formatted_code}...")
            search_box = WebDriverWait(driver, 15).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "input[placeholder*='Course Search']"))
            )
            search_box.clear()
            search_box.send_keys(formatted_code)
            
            search_button = WebDriverWait(driver, 15).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "button#button-addon2"))
            )
            search_button.click()

        except Exception as e:
            print(f"üî• Error occured with course search: {str(e)}")
            print(f"üåç Current Error URL: {driver.current_url}")
            continue

        # See if there are results or not
        try:
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr"))
            )
        except TimeoutException:
            print("‚ùå No results found, skipping...")
            continue

        # Click view button to go to syllabus
        print("üîó Accessing syllabus...")
        try:
            view_button = WebDriverWait(driver, 15).until(
                EC.element_to_be_clickable((By.XPATH, "//a[contains(@class, 'btn-outline-primary') and contains(@title, 'View Online')]"))
            )
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", view_button)
            time.sleep(0.5)
            view_button.click()
        except (TimeoutException, NoSuchElementException):
            print("‚ùå View button not found, skipping...")
            continue


        # Switch tabs and extract
        print("üîÑ Switching to new tab...")
        
        try:
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "table tbody tr"))
            )

            print("üñ®Ô∏è Parsing...")
            
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(1)
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            topics, times = extract_syllabus_data(soup)
            
            # Validation check
            if len(topics) == 0:
                print("‚ö†Ô∏è Warning: No topics extracted")
            else:
                print(f"‚úîÔ∏è Success: Collected {len(topics)} topics")
            
            syllabus_data.append({
                "course_id": course["course_id"],
                "course_code": course["course_code"],
                "course_name": course["course_name"],
                "topics": topics,
                "time": times
            })

        except TimeoutException:
            print("‚ùå Syllabus content not found")
        finally:
            print("üìÇ Closing tab...")
            driver.close()



    # Save results
    with open('syllabi.json', 'w') as f:
        json.dump(syllabus_data, f, indent=4)
    
    driver.quit()
    print(f"\nüéâ Completed: {len(syllabus_data)} syllabi processed")
    print("üíæ Output saved to syllabi.json")

if __name__ == "__main__":
    scrape_syllabi()
=======
# url='https://uwaterloo.ca/academic-calendar/undergraduate-studies/catalog#/programs/H1zle10Cs3?searchTerm=sof&bc=true&bcCurrent=Software%20Engineering%20(Bachelor%20of%20Software%20Engineering%20-%20Honours)&bcItemType=programs'

# print(courses(url))
# print(courses("https://uwaterloo.ca/academic-calendar/undergraduate-studies/catalog#/programs/H1-0kyACi3?searchTerm=physics&bc=true&bcCurrent=Physics%20(Bachelor%20of%20Science%20-%20Honours)&bcItemType=programs"))


import json

# Open and load the JSON file
with open('Hyperloo/scrapers/majorscraper/majors.json', 'r') as file:
    data = json.load(file)

programCourses = []

for i in data:
    url = i["link"]
    temp = courses(url)
    programCourses.append({
        "major_name": i["major_name"],
                    "link": i["link"],
                    "major_id": i["major_id"],
                    "program_id": i["program_id"],
                    "program_name": i["program_name"],
                    "courses":temp
    })
    
with open('Hyperloo/scrapers/scrapesyllabus/majorCourses.json', 'w') as file:
        json.dump(programCourses, file, indent=4)
    
>>>>>>> 02da7fde48ea5bbb8216f94a41c160796c560a5f
